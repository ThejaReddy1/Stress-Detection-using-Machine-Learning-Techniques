# -*- coding: utf-8 -*-
"""project2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1c4O663M-DKc9GnVI5I4KnPr8uQ-qLMVH
"""

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load
 
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
 
# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory
 
import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))
 
# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

from google.colab import drive
drive.mount('/content/drive/',force_remount=True)

df = pd.read_csv('/content/drive/MyDrive/framingham (1).csv')

from sklearn.svm import SVC
from sklearn import datasets
from sklearn.preprocessing import StandardScaler
import numpy as np

#import numpy as np
import pandas as pd
import matplotlib.pyplot as plt 
df

df.describe()

df = pd.read_csv('/content/drive/MyDrive/framingham (1).csv')

df.shape

df.head()

df['heartRate']

df['sysBP']

df['diaBP']

# check for missing values as percentage
df.isnull().sum()*100/len(df)

# Glucose has about 9% missing. Overall very small number of missing. Dropping the missing rows

df.dropna(inplace=True)

# cross check if all missing values are gone
df.isnull().sum().sum()

df.columns

# education can not have relation with CHD. So dropping it

df.drop(['education'], axis=1, inplace=True)

df.info()

df.male.value_counts()

df.currentSmoker.value_counts()

df.BPMeds.value_counts()

df.prevalentStroke.value_counts()

df.prevalentHyp.value_counts()

df.diabetes.value_counts()

df.info()

df['gender']=df['male'].map({0:'female', 1:'male'})
df['currentSmoker'] = df['currentSmoker'].astype('object')
df['prevalentHyp']=df['prevalentHyp'].astype('object')
df['prevalentStroke']=df['prevalentStroke'].astype('object')
df['diabetes']=df['diabetes'].astype('object')
df['BPMeds']=df['BPMeds'].astype('object')

df.drop(['male'], axis=1, inplace=True)
df.info()

num = list(set(df.describe().columns)-set(['TenYearCHD']))

num

df = df[df['BMI']<50]
df = df[df['sysBP']<250]
df = df[df['totChol']<500]
df = df[df['cigsPerDay']<50]

# shape after outlier treatment

df.shape

df.info()

len(df)*100/len(df)

df1 = df[df.TenYearCHD==1]
df0 = df[df.TenYearCHD==0]
df0_sample = df0.sample(n=len(df1), random_state=0)

len(df1)

len(df0_sample)

df_resampled = pd.concat([df0_sample, df1], axis=0)
df_resampled.shape

X = df_resampled.drop(['TenYearCHD'], axis=1)
y = df_resampled['TenYearCHD']

df_resampled.head()

df_resampled.info()

x=df.iloc[:,11].values
y=df.iloc[:,13].values

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.25)

x_train=x_train.reshape(-1,1)
y_train=y_train.reshape(-1,1)
x_test=x_test.reshape(-1,1)

def models(x_train,y_train):
 
 from sklearn.ensemble import RandomForestClassifier
 forest=RandomForestClassifier(n_estimators=7,criterion='entropy',random_state=0)
 forest.fit(x_train,y_train)


 from sklearn.neural_network import MLPClassifier
 clf=MLPClassifier(solver='lbfgs',hidden_layer_sizes=(5,1), random_state=0)
 clf.fit(x_train,y_train)


 print('forest    ',forest.score(x_train,y_train))
 print('clf       ',clf.score(x_train,y_train))
 

 return forest,clf

model=models(x_train,y_train)

from sklearn.tree import DecisionTreeClassifier
tree=DecisionTreeClassifier(criterion='entropy',random_state=0)
tree.fit(x_train,y_train)
from sklearn.ensemble import RandomForestClassifier
forest=RandomForestClassifier(n_estimators=10,criterion='entropy',random_state=0)
forest.fit(x_train,y_train)
estimator =forest.estimators_[5]

from sklearn.tree import export_graphviz
export_graphviz(tree, out_file='tree.dot',rounded = True, proportion = False,precision = 2, filled = True)

from subprocess import call
call(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600'])

from IPython.display import Image
Image(filename = 'tree.png')

from sklearn.metrics import confusion_matrix    

for i in range(len(model)):
  print('model',i);
  y_predict=model[i].predict(x_test)
  cm=confusion_matrix(y_test,y_predict)

  print(cm)
  tp=cm[0][0]
  tn=cm[1][1]
  fp=cm[1][0]
  fn=cm[0][1]

  print('accuracy: ',(tp+tn)/(tp+tn+fp+fn))
  print('sensitivity: ',tp/(tp+fn))
  print('specificity: ',tn/(fp+tn))
  print()

from sklearn.datasets import make_blobs
from sklearn.model_selection import train_test_split
import numpy as np
import matplotlib.pyplot as plt

# Configuration options
blobs_random_seed = 490
centers = [(0,0), (5,5)]
cluster_std = 1
frac_test_split = 0.33
num_features_for_samples = 2
num_samples_total = 490

# Generate data
inputs, targets = make_blobs(n_samples = num_samples_total, centers = centers, n_features = num_features_for_samples, cluster_std = cluster_std)
X_train, X_test, y_train, y_test = train_test_split(inputs, targets, test_size=frac_test_split, random_state=blobs_random_seed)

# Save and load temporarily
np.save('./data.npy', (X_train, X_test, y_train, y_test))
X_train, X_test, y_train, y_test = np.load('./data.npy', allow_pickle=True)

# Generate scatter plot for training data 
plt.scatter(X_train[:,0], X_train[:,1])
plt.title('Linearly separable data')
plt.xlabel('data')
plt.ylabel('2D representation')
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs
from sklearn.svm import LinearSVC

X, y = make_blobs(n_samples=490, centers=2, random_state=0)

plt.figure(figsize=(10, 5))
for i, C in enumerate([1, 400]):
    # "hinge" is the standard SVM loss
    clf = LinearSVC(C=C, loss="hinge", random_state=42).fit(X, y)
    # obtain the support vectors through the decision function
    decision_function = clf.decision_function(X)
    # we can also calculate the decision function manually
    # decision_function = np.dot(X, clf.coef_[0]) + clf.intercept_[0]
    # The support vectors are the samples that lie within the margin
    # boundaries, whose size is conventionally constrained to 1
    support_vector_indices = np.where(
        np.abs(decision_function) <= 1 + 1e-15)[0]
    support_vectors = X[support_vector_indices]

    plt.subplot(1, 2, i + 1)
    plt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=plt.cm.Paired)
    ax = plt.gca()
    xlim = ax.get_xlim()
    ylim = ax.get_ylim()
    xx, yy = np.meshgrid(np.linspace(xlim[0], xlim[1], 50),
                         np.linspace(ylim[0], ylim[1], 50))
    Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    plt.contour(xx, yy, Z, colors='k', levels=[-1, 0, 1], alpha=0.5,
                linestyles=['--', '-', '--'])
    plt.scatter(support_vectors[:, 0], support_vectors[:, 1], s=100,
                linewidth=1, facecolors='none', edgecolors='k')
    plt.xlabel('vectors representation of with mental stress and without metal stress')
    plt.ylabel('2D representation space')
    plt.title("C=" + str(C))
#plt.tight_layout()
plt.xlabel('vectors representation of with mental stress and without mental stress')
plt.ylabel('2D representation space')
plt.show()

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=100)

# Standarize features
scaler = StandardScaler()
X_std = scaler.fit_transform(X)

# Create support vector classifier object
svc = SVC(kernel='linear', random_state=0)

# Train classifier
model = svc.fit(X_train, y_train)

# View support vectors
model.support_vectors_

# View indices of support vectors
model.support_

# View number of support vectors for each class
model.n_support_

x_train=x_train.reshape(-1,1)
y_train=y_train.reshape(-1,1)
x_test=x_test.reshape(-1,1)

from sklearn.neighbors import KNeighborsClassifier
knn=KNeighborsClassifier(n_neighbors=15)
knn.fit(X_train,y_train)
print('knn',knn.score(X_train,y_train))

from sklearn.naive_bayes import GaussianNB
gnb=GaussianNB()
gnb.fit(X_train,y_train)
print('gnb',gnb.score(X_train,y_train))



